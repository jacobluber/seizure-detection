{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import tensorflow as tf\n",
    "import torch\n",
    "#import keras\n",
    "#import keras.backend as K\n",
    "#from keras.optimizers import SGD\n",
    "import matplotlib.pyplot as plt\n",
    "#from keras.models import load_model\n",
    "import torch\n",
    "#from torchsummary import summary\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "\n",
    "latent_space=16\n",
    "\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/16/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/16'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/16'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/16'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/16'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/16'+'/yVal.npy')\n",
    "# x=np.load('tmp/old/xTrain.npy')\n",
    "# y=np.load('tmp/old/yTrain.npy').reshape((x.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have your features (X) and labels (y)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "# Convert y_train to integers\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train.flatten())\n",
    "class_labels = np.unique(y_train)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_labels, class_counts)\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(class_labels)\n",
    "plt.show()\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train)\n",
    "class_weights = {class_label: total_samples / count for class_label, count in class_distribution.items()}\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Preprocess the data (scaling) //Changes made by Neel\n",
    "# scaler = StandardScaler()\n",
    "# x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "\n",
    "# Create an XGBoost classifier with class weights\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=75 / 25 , # Set scale_pos_weight based on class weights\n",
    "    device='gpu')\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier.fit(x_train_scaled, y_train.ravel())\n",
    "\n",
    "# # You can continue with validation and testing as before\n",
    "\n",
    "# Evaluate on the validation set\n",
    "\n",
    "accuracy_valid = xgb_classifier.score(x_valid_scaled, y_valid)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy_test = xgb_classifier.score(x_test_scaled, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_classifier.predict(x_test_scaled)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define class labels (modify as needed)\n",
    "class_labels = [\"No Seizure\", \"Seizure\"]\n",
    "\n",
    "annot_kws = {\"fontsize\": 22, \"fontweight\": \"bold\"}\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels,  annot_kws=annot_kws)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix 16D Latent Space')\n",
    "plt.savefig('CM_16.jpeg',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############onchip\n",
    "\n",
    "# Load the data\n",
    "\n",
    "latent_space=16\n",
    "\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/16/onchip/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/16/onchip'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/16/onchip'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/16/onchip'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/16/onchip'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/32/onchip'+'/yVal.npy')\n",
    "# x=np.load('tmp/old/xTrain.npy')\n",
    "# y=np.load('tmp/old/yTrain.npy').reshape((x.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have your features (X) and labels (y)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "# Convert y_train to integers\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train.flatten())\n",
    "class_labels = np.unique(y_train)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_labels, class_counts)\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(class_labels)\n",
    "plt.show()\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train)\n",
    "class_weights = {class_label: total_samples / count for class_label, count in class_distribution.items()}\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Preprocess the data (scaling) //Changes made by Neel\n",
    "# scaler = StandardScaler()\n",
    "# x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "\n",
    "# Create an XGBoost classifier with class weights\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=75 / 25 , # Set scale_pos_weight based on class weights\n",
    "    device='gpu')\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier.fit(x_train_scaled, y_train.ravel())\n",
    "\n",
    "# # You can continue with validation and testing as before\n",
    "\n",
    "# Evaluate on the validation set\n",
    "\n",
    "accuracy_valid = xgb_classifier.score(x_valid_scaled, y_valid)\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy_test = xgb_classifier.score(x_test_scaled, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_classifier.predict(x_test_scaled)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define class labels (modify as needed)\n",
    "class_labels = [\"No Seizure\", \"Seizure\"]\n",
    "\n",
    "annot_kws = {\"fontsize\": 22, \"fontweight\": \"bold\"}\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels,  annot_kws=annot_kws)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix 16D Latent Space')\n",
    "plt.savefig('CM_16.jpeg',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "\n",
    "latent_space=32\n",
    "\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/32/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/32'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/32'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/32'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/32'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/32'+'/yVal.npy')\n",
    "\n",
    "\n",
    "# x=np.load('tmp/old/xTrain.npy')\n",
    "# y=np.load('tmp/old/yTrain.npy').reshape((x.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have your features (X) and labels (y)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled_32 = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "# Convert y_train to integers\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train.flatten())\n",
    "class_labels = np.unique(y_train)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_labels, class_counts)\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(class_labels)\n",
    "plt.show()\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train)\n",
    "class_weights = {class_label: total_samples / count for class_label, count in class_distribution.items()}\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Preprocess the data (scaling) //Changes made by Neel\n",
    "# scaler = StandardScaler()\n",
    "# x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "\n",
    "# Create an XGBoost classifier with class weights\n",
    "xgb_classifier_32 = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=75 / 25 , # Set scale_pos_weight based on class weights\n",
    "    device='gpu'\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier_32.fit(x_train_scaled, y_train.ravel())\n",
    "\n",
    "# # You can continue with validation and testing as before\n",
    "\n",
    "# Evaluate on the validation set\n",
    "\n",
    "accuracy_valid = xgb_classifier_32.score(x_valid_scaled, y_valid)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy_test = xgb_classifier_32.score(x_test_scaled_32, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_classifier_32.predict(x_test_scaled_32)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define class labels (modify as needed)\n",
    "class_labels = [\"No Seizure\", \"Seizure\"]\n",
    "\n",
    "annot_kws = {\"fontsize\": 22, \"fontweight\": \"bold\"}\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels,  annot_kws=annot_kws)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix 32D Latent Space')\n",
    "plt.savefig('CM_32.jpeg',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the data\n",
    "###on the chip\n",
    "import joblib\n",
    "latent_space=32\n",
    "\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32'+'/yVal.npy')\n",
    "\n",
    "\n",
    "# x=np.load('tmp/old/xTrain.npy')\n",
    "# y=np.load('tmp/old/yTrain.npy').reshape((x.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have your features (X) and labels (y)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled_32 = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "# Convert y_train to integers\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train.flatten())\n",
    "class_labels = np.unique(y_train)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_labels, class_counts)\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(class_labels)\n",
    "plt.show()\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train)\n",
    "class_weights = {class_label: total_samples / count for class_label, count in class_distribution.items()}\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Preprocess the data (scaling) //Changes made by Neel\n",
    "# scaler = StandardScaler()\n",
    "# x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "\n",
    "# Create an XGBoost classifier with class weights\n",
    "xgb_classifier_32 = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=75 / 25 , # Set scale_pos_weight based on class weights\n",
    "    device='gpu'\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier_32.fit(x_train_scaled, y_train.ravel())\n",
    "\n",
    "# # You can continue with validation and testing as before\n",
    "\n",
    "\n",
    "# Define the file path where you want to save the model\n",
    "model_filename = \"/home/axh5735/projects/signal_compression/xgb_classifier_32_model5.pkl\"\n",
    "\n",
    "# Save the model to disk\n",
    "#joblib.dump(xgb_classifier_32, model_filename)\n",
    "with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(xgb_classifier_32, f)\n",
    "\n",
    "\n",
    "# Evaluate on the validation set\n",
    "\n",
    "accuracy_valid = xgb_classifier_32.score(x_valid_scaled, y_valid)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy_test = xgb_classifier_32.score(x_test_scaled_32, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_classifier_32.predict(x_test_scaled_32)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define class labels (modify as needed)\n",
    "class_labels = [\"No Seizure\", \"Seizure\"]\n",
    "\n",
    "annot_kws = {\"fontsize\": 22, \"fontweight\": \"bold\"}\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels,  annot_kws=annot_kws)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix 32D Latent Space')\n",
    "plt.savefig('CM_32.jpeg',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Define the file path where the model is saved\n",
    "model_filename = \"xgb_classifier_32_model.pkl\"\n",
    "\n",
    "# Load the model from disk\n",
    "loaded_model = joblib.load(model_filename)\n",
    "\n",
    "accuracy_test = loaded_model.score(x_test_scaled_32, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "\n",
    "latent_space=64\n",
    "\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/64/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/64'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/64'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/64'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/64'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/64'+'/yVal.npy')\n",
    "\n",
    "# x=np.load('tmp/old/xTrain.npy')\n",
    "# y=np.load('tmp/old/yTrain.npy').reshape((x.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have your features (X) and labels (y)\n",
    "# x_train, x_t, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled_64 = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "# Convert y_train to integers\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train.flatten())\n",
    "class_labels = np.unique(y_train)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_labels, class_counts)\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(class_labels)\n",
    "plt.show()\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train)\n",
    "class_weights = {class_label: total_samples / count for class_label, count in class_distribution.items()}\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Preprocess the data (scaling) //Changes made by Neel\n",
    "# scaler = StandardScaler()\n",
    "# x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "\n",
    "# Create an XGBoost classifier with class weights\n",
    "xgb_classifier_64 = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=75 / 25 , # Set scale_pos_weight based on class weights\n",
    "    device='gpu'\n",
    "    \n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier_64.fit(x_train_scaled, y_train.ravel())\n",
    "\n",
    "# # You can continue with validation and testing as before\n",
    "\n",
    "# Evaluate on the validation set\n",
    "\n",
    "accuracy_valid = xgb_classifier_64.score(x_valid_scaled, y_valid)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy_test = xgb_classifier_64.score(x_test_scaled_64, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_classifier_64.predict(x_test_scaled_64)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define class labels (modify as needed)\n",
    "class_labels = [\"No Seizure\", \"Seizure\"]\n",
    "\n",
    "annot_kws = {\"fontsize\": 22, \"fontweight\": \"bold\"}\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels,  annot_kws=annot_kws)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix 64D Latent Space')\n",
    "plt.savefig('CM_64.jpeg',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "#####on the chip\n",
    "\n",
    "latent_space=64\n",
    "\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/64/onchip/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/64/onchip'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/64/onchip'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/64/onchip'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/64/onchip'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/64/onchip'+'/yVal.npy')\n",
    "\n",
    "# x=np.load('tmp/old/xTrain.npy')\n",
    "# y=np.load('tmp/old/yTrain.npy').reshape((x.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have your features (X) and labels (y)\n",
    "# x_train, x_t, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled_64 = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "# Convert y_train to integers\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train.flatten())\n",
    "class_labels = np.unique(y_train)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_labels, class_counts)\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(class_labels)\n",
    "plt.show()\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train)\n",
    "class_weights = {class_label: total_samples / count for class_label, count in class_distribution.items()}\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Preprocess the data (scaling) //Changes made by Neel\n",
    "# scaler = StandardScaler()\n",
    "# x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "\n",
    "# Create an XGBoost classifier with class weights\n",
    "xgb_classifier_64 = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=75 / 25 , # Set scale_pos_weight based on class weights\n",
    "    device='gpu'\n",
    "    \n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier_64.fit(x_train_scaled, y_train.ravel())\n",
    "\n",
    "# # You can continue with validation and testing as before\n",
    "\n",
    "# Evaluate on the validation set\n",
    "\n",
    "accuracy_valid = xgb_classifier_64.score(x_valid_scaled, y_valid)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy_test = xgb_classifier_64.score(x_test_scaled_64, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_classifier_64.predict(x_test_scaled_64)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define class labels (modify as needed)\n",
    "class_labels = [\"No Seizure\", \"Seizure\"]\n",
    "\n",
    "annot_kws = {\"fontsize\": 22, \"fontweight\": \"bold\"}\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels,  annot_kws=annot_kws)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix 64D Latent Space')\n",
    "plt.savefig('CM_64.jpeg',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "\n",
    "latent_space=128\n",
    "\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/128/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/128'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/128'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/128'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/128'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/128'+'/yVal.npy')\n",
    "\n",
    "\n",
    "\n",
    "# x=np.load('tmp/old/xTrain.npy')\n",
    "# y=np.load('tmp/old/yTrain.npy').reshape((x.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have your features (X) and labels (y)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled_128 = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "# Convert y_train to integers\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train.flatten())\n",
    "class_labels = np.unique(y_train)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_labels, class_counts)\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(class_labels)\n",
    "plt.show()\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train)\n",
    "class_weights = {class_label: total_samples / count for class_label, count in class_distribution.items()}\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Preprocess the data (scaling) //Changes made by Neel\n",
    "# scaler = StandardScaler()\n",
    "# x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "\n",
    "# Create an XGBoost classifier with class weights\n",
    "xgb_classifier_128 = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=75 / 25 , # Set scale_pos_weight based on class weights\n",
    "    device='gpu'\n",
    "    #tree_method='gpu_hist'\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier_128.fit(x_train_scaled, y_train.ravel())\n",
    "\n",
    "# # You can continue with validation and testing as before\n",
    "\n",
    "# Evaluate on the validation set\n",
    "\n",
    "accuracy_valid = xgb_classifier_128.score(x_valid_scaled, y_valid)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy_test = xgb_classifier_128.score(x_test_scaled_128, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_classifier_128.predict(x_test_scaled_128)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define class labels (modify as needed)\n",
    "class_labels = [\"No Seizure\", \"Seizure\"]\n",
    "\n",
    "annot_kws = {\"fontsize\": 22, \"fontweight\": \"bold\"}\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels,  annot_kws=annot_kws)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix 128D Latent Space')\n",
    "plt.savefig('CM_128.jpeg',dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "###on the chip\n",
    "\n",
    "latent_space=128\n",
    "\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/128/onchip/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/128/onchip'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/128/onchip'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/128/onchip'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/128/onchip'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/128/onchip'+'/yVal.npy')\n",
    "\n",
    "\n",
    "\n",
    "# x=np.load('tmp/old/xTrain.npy')\n",
    "# y=np.load('tmp/old/yTrain.npy').reshape((x.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have your features (X) and labels (y)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled_128 = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "# Convert y_train to integers\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train.flatten())\n",
    "class_labels = np.unique(y_train)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_labels, class_counts)\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(class_labels)\n",
    "plt.show()\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train)\n",
    "class_weights = {class_label: total_samples / count for class_label, count in class_distribution.items()}\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Preprocess the data (scaling) //Changes made by Neel\n",
    "# scaler = StandardScaler()\n",
    "# x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "\n",
    "# Create an XGBoost classifier with class weights\n",
    "xgb_classifier_128 = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=75 / 25 , # Set scale_pos_weight based on class weights\n",
    "    device='gpu'\n",
    "    #tree_method='gpu_hist'\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier_128.fit(x_train_scaled, y_train.ravel())\n",
    "\n",
    "# # You can continue with validation and testing as before\n",
    "\n",
    "# Evaluate on the validation set\n",
    "\n",
    "accuracy_valid = xgb_classifier_128.score(x_valid_scaled, y_valid)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy_test = xgb_classifier_128.score(x_test_scaled_128, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_classifier_128.predict(x_test_scaled_128)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define class labels (modify as needed)\n",
    "class_labels = [\"No Seizure\", \"Seizure\"]\n",
    "\n",
    "annot_kws = {\"fontsize\": 22, \"fontweight\": \"bold\"}\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels,  annot_kws=annot_kws)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix 128D Latent Space')\n",
    "plt.savefig('CM_128.jpeg',dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "\n",
    "latent_space=256\n",
    "\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/256/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/256'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/256'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/256'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/256'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/256'+'/yVal.npy')\n",
    "\n",
    "\n",
    "# x=np.load('tmp/old/xTrain.npy')\n",
    "# y=np.load('tmp/old/yTrain.npy').reshape((x.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have your features (X) and labels (y)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled_256 = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "# Convert y_train to integers\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train.flatten())\n",
    "class_labels = np.unique(y_train)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_labels, class_counts)\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(class_labels)\n",
    "plt.show()\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train)\n",
    "class_weights = {class_label: total_samples / count for class_label, count in class_distribution.items()}\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Preprocess the data (scaling) //Changes made by Neel\n",
    "# scaler = StandardScaler()\n",
    "# x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "\n",
    "# Create an XGBoost classifier with class weights\n",
    "xgb_classifier_256 = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=75 / 25 , # Set scale_pos_weight based on class weights\n",
    "    device='gpu'\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier_256.fit(x_train_scaled, y_train.ravel())\n",
    "\n",
    "# # You can continue with validation and testing as before\n",
    "\n",
    "# Evaluate on the validation set\n",
    "\n",
    "accuracy_valid = xgb_classifier_256.score(x_valid_scaled, y_valid)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy_test = xgb_classifier_256.score(x_test_scaled_256, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_classifier_256.predict(x_test_scaled_256)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define class labels (modify as needed)\n",
    "class_labels = [\"No Seizure\", \"Seizure\"]\n",
    "\n",
    "sns.set(font_scale=1.2)  # Set font scale for all text in the plot\n",
    "\n",
    "# Increase font size and make it bold for annotations (numbers in cells)\n",
    "annot_kws = {\"fontsize\": 22, \"fontweight\": \"bold\"}\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels,  annot_kws=annot_kws)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix 256D Latent Space')\n",
    "plt.savefig('CM_256.jpeg',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "###on the chip\n",
    "\n",
    "latent_space=256\n",
    "\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/256/onchip/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/256/onchip'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/256/onchip'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/256/onchip'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/256/onchip'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/256/onchip'+'/yVal.npy')\n",
    "\n",
    "\n",
    "# x=np.load('tmp/old/xTrain.npy')\n",
    "# y=np.load('tmp/old/yTrain.npy').reshape((x.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "# # Assuming you have your features (X) and labels (y)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled_256 = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "# Convert y_train to integers\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train.flatten())\n",
    "class_labels = np.unique(y_train)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Plot class distribution\n",
    "plt.bar(class_labels, class_counts)\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(class_labels)\n",
    "plt.show()\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train)\n",
    "class_weights = {class_label: total_samples / count for class_label, count in class_distribution.items()}\n",
    "\n",
    "print(class_weights)\n",
    "\n",
    "# Preprocess the data (scaling) //Changes made by Neel\n",
    "# scaler = StandardScaler()\n",
    "# x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "\n",
    "# Create an XGBoost classifier with class weights\n",
    "xgb_classifier_256 = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=75 / 25 , # Set scale_pos_weight based on class weights\n",
    "    device='gpu'\n",
    ")\n",
    "\n",
    "# Train the classifier\n",
    "xgb_classifier_256.fit(x_train_scaled, y_train.ravel())\n",
    "\n",
    "# # You can continue with validation and testing as before\n",
    "\n",
    "# Evaluate on the validation set\n",
    "\n",
    "accuracy_valid = xgb_classifier_256.score(x_valid_scaled, y_valid)\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_valid)\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy_test = xgb_classifier_256.score(x_test_scaled_256, y_test)\n",
    "print(\"Test Accuracy:\", accuracy_test)\n",
    "\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = xgb_classifier_256.predict(x_test_scaled_256)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Define class labels (modify as needed)\n",
    "class_labels = [\"No Seizure\", \"Seizure\"]\n",
    "\n",
    "sns.set(font_scale=1.2)  # Set font scale for all text in the plot\n",
    "\n",
    "# Increase font size and make it bold for annotations (numbers in cells)\n",
    "annot_kws = {\"fontsize\": 22, \"fontweight\": \"bold\"}\n",
    "# Create a heatmap for the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels,  annot_kws=annot_kws)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix 256D Latent Space')\n",
    "plt.savefig('CM_256.jpeg',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import catplot\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "latent_space=32\n",
    "\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/32/onchip_32'+'/yVal.npy')\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "\n",
    "# Convert y_train and y_valid to integers\n",
    "y_train_int = y_train.astype(np.int64)\n",
    "y_valid_int = y_valid.astype(np.int64)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train_int.flatten())\n",
    "class_labels = np.unique(y_train_int)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train_int)\n",
    "class_weights = {class_label: total_samples / (2 * count) for class_label, count in class_distribution.items()}\n",
    "\n",
    "# Initialize an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=class_weights[1] / class_weights[0],  # Set scale_pos_weight based on class weights\n",
    "    device='gpu',\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "k_folds = 10\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Train and validate using k-fold cross-validation\n",
    "avg_valid_accuracy = 0\n",
    "\n",
    "precision, recall,auc_score = [],[],[]\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(x_train_scaled, y_train_int)):\n",
    "    x_train_fold, x_valid_fold = x_train_scaled[train_idx], x_train_scaled[valid_idx]\n",
    "    y_train_fold, y_valid_fold = y_train_int[train_idx], y_train_int[valid_idx]\n",
    "\n",
    "    # Train the XGBoost classifier\n",
    "    xgb_classifier.fit(x_train_fold, y_train_fold, eval_set=[(x_valid_fold, y_valid_fold)],\n",
    "                       verbose=False)\n",
    "\n",
    "    valid_predictions = xgb_classifier.predict(x_valid_fold)\n",
    "    valid_accuracy = accuracy_score(y_valid_fold, valid_predictions)\n",
    "    avg_valid_accuracy += valid_accuracy\n",
    "    print(f\"Fold {fold + 1} - Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "    y_pred_prob=xgb_classifier.predict_proba(x_valid_fold)[:, 1]\n",
    "    p,r,_=precision_recall_curve(y_valid_fold, y_pred_prob)\n",
    "    precision.append(p)\n",
    "    recall.append(r)\n",
    "# Calculate area under the precision-recall curve\n",
    "    auc_score.append(auc(r, p))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "\n",
    "font = {'family': 'serif',\n",
    "        'color':  'black',\n",
    "        'weight': 'bold',\n",
    "        'size': 14,\n",
    "        }\n",
    "\n",
    "# Define a list of pastel colors\n",
    "pastel_colors = ['#FFB6C1', '#FFD700', '#87CEEB', '#98FB98', '#FFA07A', '#9370DB', '#00CED1', '#FF6347', '#FF69B4', '#00FA9A']\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.figure(figsize=(9, 6))\n",
    "for i in range(10):\n",
    "    # Make sure x-values are unique\n",
    "    unique_recall, unique_indices = np.unique(recall[i], return_index=True)\n",
    "    unique_precision = [precision[i][index] for index in unique_indices]\n",
    "    \n",
    "    # Smooth the recall and precision values using spline interpolation\n",
    "    recall_smooth = np.linspace(unique_recall.min(), unique_recall.max(), 300)\n",
    "    precision_smooth = make_interp_spline(unique_recall, unique_precision)(recall_smooth)\n",
    "    if i==0:\n",
    "        plt.plot(recall_smooth, precision_smooth, color=pastel_colors[i], linewidth=3, label=f'auc_score for {i+1}st fold = {auc_score[i]:.2f}')\n",
    "    elif i==1:\n",
    "        plt.plot(recall_smooth, precision_smooth, color=pastel_colors[i], linewidth=3, label=f'auc_score for {i+1}nd fold = {auc_score[i]:.2f}')\n",
    "    elif i==2:\n",
    "        plt.plot(recall_smooth, precision_smooth, color=pastel_colors[i], linewidth=3, label=f'auc_score for {i+1}rd fold = {auc_score[i]:.2f}')\n",
    "    else:\n",
    "        plt.plot(recall_smooth, precision_smooth, color=pastel_colors[i], linewidth=3, label=f'auc_score for {i+1}s fold = {auc_score[i]:.2f}')\n",
    "plt.plot([0, 1], [0.3, 0.3], color='#CCCCCC', linestyle='--',linewidth=3 )\n",
    "plt.xlabel('Recall', fontdict=font)\n",
    "plt.ylabel('Precision', fontdict=font)\n",
    "plt.title('Precision-Recall Curve for 32D Latent Space', fontdict=font)\n",
    "plt.legend(loc='center left', fontsize=14, bbox_to_anchor=(0.03, 0.55))\n",
    "plt.grid(True, alpha=0.3, linestyle='-')  # Adjust the alpha parameter for transparency\n",
    "plt.ylim(0.2, 1.05)\n",
    "plt.yticks(fontsize=14) \n",
    "plt.xticks(fontsize=14) \n",
    "plt.tight_layout()\n",
    "plt.savefig('k-fold_PR.jpeg', dpi=1200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import catplot\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "latent_space = 64\n",
    "x_train = np.load('/home/axh5735/projects/signal_compression/task/64/onchip/xTrain.npy')\n",
    "y_train = np.load('/home/axh5735/projects/signal_compression/task/64/onchip'+'/yTrain.npy')\n",
    "x_test = np.load('/home/axh5735/projects/signal_compression/task/64/onchip'+'/xTest.npy')\n",
    "y_test = np.load('/home/axh5735/projects/signal_compression/task/64/onchip'+'/yTest.npy')\n",
    "x_valid = np.load('/home/axh5735/projects/signal_compression/task/64/onchip'+'/xVal.npy')\n",
    "y_valid = np.load('/home/axh5735/projects/signal_compression/task/64/onchip'+'/yVal.npy')\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.reshape(x_train.shape[0], -1))\n",
    "x_test_scaled = scaler.transform(x_test.reshape(x_test.shape[0], -1))\n",
    "x_valid_scaled = scaler.transform(x_valid.reshape(x_valid.shape[0], -1))\n",
    "\n",
    "# Convert y_train and y_valid to integers\n",
    "y_train_int = y_train.astype(np.int64)\n",
    "y_valid_int = y_valid.astype(np.int64)\n",
    "\n",
    "# Calculate class distribution\n",
    "class_counts = np.bincount(y_train_int.flatten())\n",
    "class_labels = np.unique(y_train_int)\n",
    "class_distribution = dict(zip(class_labels, class_counts))\n",
    "\n",
    "# Calculate class weights manually\n",
    "total_samples = len(y_train_int)\n",
    "class_weights = {class_label: total_samples / (2 * count) for class_label, count in class_distribution.items()}\n",
    "\n",
    "# Initialize an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=class_weights[1] / class_weights[0],  # Set scale_pos_weight based on class weights\n",
    "    tree_method='gpu_hist',\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "k_folds = 10\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Train and validate using k-fold cross-validation\n",
    "avg_valid_accuracy = 0\n",
    "\n",
    "precision, recall,auc_score = [],[],[]\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(x_train_scaled, y_train_int)):\n",
    "    x_train_fold, x_valid_fold = x_train_scaled[train_idx], x_train_scaled[valid_idx]\n",
    "    y_train_fold, y_valid_fold = y_train_int[train_idx], y_train_int[valid_idx]\n",
    "\n",
    "    # Train the XGBoost classifier\n",
    "    xgb_classifier.fit(x_train_fold, y_train_fold, eval_set=[(x_valid_fold, y_valid_fold)],\n",
    "                       verbose=False)\n",
    "\n",
    "    valid_predictions = xgb_classifier.predict(x_valid_fold)\n",
    "    valid_accuracy = accuracy_score(y_valid_fold, valid_predictions)\n",
    "    avg_valid_accuracy += valid_accuracy\n",
    "    print(f\"Fold {fold + 1} - Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "    y_pred_prob=xgb_classifier.predict_proba(x_valid_fold)[:, 1]\n",
    "    #p,r,_=precision_recall_curve(y_valid_fold, y_pred_prob)\n",
    "    p,r,_=roc_curve(y_valid_fold, y_pred_prob)\n",
    "    precision.append(p)\n",
    "    recall.append(r)\n",
    "# Calculate area under the precision-recall curve\n",
    "    auc_score.append(auc(p, r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "\n",
    "font = {'family': 'serif',\n",
    "        'color':  'black',\n",
    "        'weight': 'bold',\n",
    "        'size': 14,\n",
    "        }\n",
    "\n",
    "# Define a list of pastel colors\n",
    "pastel_colors = ['#FFB6C1', '#FFD700', '#87CEEB', '#98FB98', '#FFA07A', '#9370DB', '#00CED1', '#FF6347', '#FF69B4', '#00FA9A']\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.figure(figsize=(9, 6))\n",
    "for i in range(10):\n",
    "    # Make sure x-values are unique\n",
    "    unique_recall, unique_indices = np.unique(recall[i], return_index=True)\n",
    "    unique_precision = [precision[i][index] for index in unique_indices]\n",
    "    \n",
    "    # Smooth the recall and precision values using spline interpolation\n",
    "    recall_smooth = np.linspace(unique_recall.min(), unique_recall.max(), 300)\n",
    "    precision_smooth = make_interp_spline(unique_recall, unique_precision)(recall_smooth)\n",
    "    if i==0:\n",
    "        plt.plot(precision_smooth,recall_smooth, color=pastel_colors[i], linewidth=3, label=f'auc_score for {i+1}st fold = {auc_score[i]:.2f}')\n",
    "    elif i==1:\n",
    "        plt.plot(precision_smooth,recall_smooth, color=pastel_colors[i], linewidth=3, label=f'auc_score for {i+1}nd fold = {auc_score[i]:.2f}')\n",
    "    elif i==2:\n",
    "        plt.plot(precision_smooth,recall_smooth, color=pastel_colors[i], linewidth=3, label=f'auc_score for {i+1}rd fold = {auc_score[i]:.2f}')\n",
    "    else:\n",
    "        plt.plot(precision_smooth,recall_smooth, color=pastel_colors[i], linewidth=3, label=f'auc_score for {i+1}th fold = {auc_score[i]:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='#CCCCCC', linestyle='--',linewidth=3 )\n",
    "plt.xlabel('False Positive rate', fontdict=font)\n",
    "plt.ylabel('True Positive rate', fontdict=font)\n",
    "plt.title('ROC Curve for 64D Latent Space', fontdict=font)\n",
    "plt.legend(loc='lower right', fontsize=14, bbox_to_anchor=(1, 0))\n",
    "plt.grid(True, alpha=0.3, linestyle='-')  # Adjust the alpha parameter for transparency\n",
    "plt.yticks(fontsize=14) \n",
    "plt.xticks(fontsize=14) \n",
    "plt.tight_layout()\n",
    "plt.savefig('k-fold_roc.jpeg', dpi=1200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob=[]\n",
    "yValPred=np.load('oldpred.npy')\n",
    "yValTrue=np.load('oldtrue.npy')\n",
    "y_pred_prob.append(xgb_classifier.predict_proba(x_test_scaled)[:, 1])\n",
    "y_pred_prob.append(xgb_classifier_32.predict_proba(x_test_scaled_32)[:, 1])\n",
    "y_pred_prob.append(xgb_classifier_64.predict_proba(x_test_scaled_64)[:, 1])\n",
    "y_pred_prob.append(xgb_classifier_128.predict_proba(x_test_scaled_128)[:, 1])\n",
    "y_pred_prob.append(xgb_classifier_256.predict_proba(x_test_scaled_256)[:, 1])\n",
    "# Calculate precision and recall values\n",
    "precision, recall,auc_score = [],[],[]\n",
    "for i in range(5):\n",
    "    p,r,_=precision_recall_curve(y_test, y_pred_prob[i])\n",
    "    unique_recall, unique_indices = np.unique(r, return_index=True)\n",
    "    unique_precision = [p[index] for index in unique_indices]\n",
    "    \n",
    "    # Smooth the recall and precision values using spline interpolation\n",
    "    recall_smooth = np.linspace(unique_recall.min(), unique_recall.max(), 300)\n",
    "    precision_smooth = make_interp_spline(unique_recall, unique_precision)(recall_smooth)\n",
    "    precision.append(precision_smooth)\n",
    "    recall.append(recall_smooth)\n",
    "# Calculate area under the precision-recall curve\n",
    "    auc_score.append( auc(r, p)) \n",
    "precision1, recall1, thresholds1 = precision_recall_curve(yValTrue, yValPred)\n",
    "auc_score1 = auc(recall1, precision1)\n",
    "font = {'family': 'serif',\n",
    "        'color':  'black',\n",
    "        'weight': 'bold',\n",
    "        'size': 14,\n",
    "        }\n",
    "pastel_colors = ['#FFB6C1', '#FFD700', '#87CEEB', '#98FB98', '#FFA07A', '#9370DB']\n",
    "# Plot the precision-recall curve\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(recall[0], precision[0], color=pastel_colors[0],linewidth=4, label=f'AUC of 16D = {auc_score[0]:.2f}')\n",
    "plt.plot(recall[1], precision[1], color=pastel_colors[1],linewidth=4, label=f'AUC of 32D = {auc_score[1]:.2f}')\n",
    "plt.plot(recall[2], precision[2], color=pastel_colors[2],linewidth=4, label=f'AUC of 64D = {auc_score[2]:.2f}')\n",
    "plt.plot(recall[3], precision[3], color=pastel_colors[3], linewidth=4,label=f'AUC of 128D = {auc_score[3]:.2f}')\n",
    "plt.plot(recall[4], precision[4], color=pastel_colors[4], linewidth=4,label=f'AUC of 256D = {auc_score[4]:.2f}')\n",
    "plt.plot(recall1, precision1, color=pastel_colors[5],linewidth=4, label=f'AUC Original = {auc_score1:.2f}')\n",
    "plt.plot([0, 1], [0.3, 0.3], color='#CCCCCC', linestyle='--',linewidth=3 )\n",
    "plt.xlabel('Recall',fontdict=font)\n",
    "plt.ylabel('Precision',fontdict=font)\n",
    "plt.title('Precision-Recall Curve for XGBoost on Seizure Detection',fontdict=font)\n",
    "plt.legend(loc='center left', fontsize=14, bbox_to_anchor=(0.03, 0.60))\n",
    "plt.grid(True, alpha=0.3, linestyle='-')  # Adjust the alpha parameter for transparency\n",
    "plt.ylim(0.2, 1.05)\n",
    "plt.yticks(fontsize=14) \n",
    "plt.xticks(fontsize=14) \n",
    "plt.tight_layout()\n",
    "plt.savefig('roc-curve_cmp.jpeg',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    bce_loss = (0.7 * -y_true * torch.log(y_pred)) + (0.3 * -(1 - y_true) * torch.log(1 - y_pred))\n",
    "    mean_bce_loss = torch.mean(bce_loss)\n",
    "    return mean_bce_loss\n",
    "\n",
    "a=[1,1,0,0]\n",
    "b=[1,1,0,0]\n",
    "for i in range(5):\n",
    "    threshold = 0.5  # Adjust this threshold as needed\n",
    "    yValPred_binary = (y_pred_prob[i] > threshold).astype(int)\n",
    "    print(custom_loss(torch.tensor(yValPred_binary),torch.tensor(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['16D (1:1170 CR)', '32D (1:585 CR)', '64D (1:293 CR)', '128D (1:146 CR)', '256D (1:73 CR)']\n",
    "values=[84, 86, 91, 87, 83]\n",
    "colors = [(0.39, 0.58, 0.93, 0.8), (0.39, 0.58, 0.93, 0.8), (0.39, 0.58, 0.93, 0.8), (0.39, 0.58, 0.93, 0.8), (0.39, 0.58, 0.93, 0.8)]\n",
    "font = {'family': 'serif',\n",
    "        'color':  'black',\n",
    "        'weight': 'bold',\n",
    "        'size': 14,\n",
    "        }\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(categories,values,color=colors)\n",
    "plt.xlabel('Latent Space Dimensions (Compression ratio)',fontdict=font)\n",
    "plt.ylabel('Detection Accuracy',fontdict=font)\n",
    "plt.title('Compression vs Accuracy',fontdict=font)\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(80, 93)  # Set y-axis limits\n",
    "plt.yticks(range(80, 93, 1),fontsize=14) \n",
    "\n",
    "plt.grid(True, alpha=0.3, linestyle='-')  # Adjust the alpha parameter for transparency\n",
    "\n",
    "plt.tight_layout()\n",
    "text_gap = 0.4\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval +  text_gap, f'{round(yval)}%', ha='center', fontsize=14)\n",
    "    plt.xticks(fontsize=14) \n",
    "\n",
    "plt.savefig('accuracy.jpeg',dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
